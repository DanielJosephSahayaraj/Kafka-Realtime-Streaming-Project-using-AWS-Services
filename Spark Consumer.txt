from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType
import boto3

# Define AWS S3 client
s3 = boto3.client('s3')

# Create a Spark Session
spark = SparkSession \
    .builder \
    .appName("Kafka-Spark-AWS") \
    .getOrCreate()

# Kafka topic and brokers
kafka_topic = "MyTopic"
kafka_bootstrap_servers = "arn:aws:kafka:us-east-1:022499033696:cluster/kafka-project/38ab44c1-e264-4fb8-bcda"

# Define the schema for incoming JSON data
schema = StructType() \
    .add("sensor_id", IntegerType()) \
    .add("temperature", DoubleType()) \
    .add("humidity", DoubleType()) \
    .add("timestamp", IntegerType())

# Read streaming data from Kafka
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", kafka_topic) \
    .option("startingOffsets", "earliest") \
    .load()

# Extract value as JSON and apply the schema
json_df = kafka_df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# Example: Filter based on temperature
filtered_df = json_df.filter(col("temperature") > 25)

# Write the filtered data to console for debugging
query = filtered_df \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

# Write the filtered data to AWS S3
def write_to_s3(batch_df, batch_id):
    records = batch_df.toJSON().collect()
    for record in records:
        # You can create a file-like structure and upload it to S3
        s3.put_object(Body=record, Bucket='kafka-test-danny', Key=f'kafka_data/{batch_id}.json')

filtered_df.writeStream.foreachBatch(write_to_s3).start()

# Await termination
query.awaitTermination()
